import os

import math

import argparse

import subprocess

import time

from cloudvolume import CloudVolume


parser = argparse.ArgumentParser(description="3D Brain Tissue Segmentation")
parser.add_argument('--data_path', type=str, required=True, 
                        help='Path to the dataset')
parser.add_argument('--mode', type=str, choices=['myelin_BV', 'soma', 'marker'], required=True, 
                        help='Mode of the segmentation')
parser.add_argument('--block_shape', nargs=3, type=int, required=True, 
                        help='Shape of the block to load, should be larger than 200')
parser.add_argument('--output_name', type=str, required=True, 
                        help='Descriptive name of the output')
parser.add_argument('--model_path', type=str, required=True, 
                        help='Path to the model')
parser.add_argument('--testing_x_loc', type=int, default=None,
                        help='x location for testing pipeline')
parser.add_argument('--export_debug', action=argparse.BooleanOptionalAction, help='Export debug information from the individual jobs')
parser.add_argument('--ds_levels', default=[2,4], nargs="+", type=int, help="mips to include for the inference")
parser.add_argument("--attention", type=bool, default=False,
                    help="Use attention gates.")
parser.add_argument('--step', type=int, default=2,
                        help='step size for overlapping blocks')
args = parser.parse_args()

t0 = time.time()

stride = [s-100 for s in args.block_shape]

if args.export_debug:
    debug_path = args.output_name + '_debug'
    if not os.path.isdir(debug_path):
        os.makedirs(debug_path)

in_vol = CloudVolume(args.data_path, progress=True)
data_shape = in_vol.info['scales'][0]['size']

if args.mode == 'myelin_BV' or args.mode == 'soma':

    volume_info = {
            "type": "segmentation",
            "layer_type": "segmentation",
            "data_type": "uint64",
            "num_channels": 1,
            "scales": [
                {
                    "voxel_offset": [0, 0, 0],
                    "key": "0_0_0",
                    "size": data_shape,
                    "resolution": [727.8, 727.8, 727.8],     # Resolution in nm
                    "chunk_sizes": [1024, 1024, 1024],
                    "encoding": "compressed_segmentation",
                    "compressed_segmentation_block_size": [8, 8, 8],
                }
            ]
        }

    out_vol = CloudVolume(args.output_name, info=volume_info, bounded=False, progress=True, non_aligned_writes=False, parallel=1)
    out_vol.provenance.description = f"Semantic segmentation ({args.mode}) generated by Pi2/Monai UNet model"
    out_vol.provenance.owners = ["jonas.hemesath@bi.mpg.de"]
    out_vol.commit_info()
    out_vol.commit_provenance()
elif args.mode == 'marker':

    volume_info = {
            "type": "image",
            "layer_type": "image",
            "data_type": "float32",
            "num_channels": 1,
            "scales": [
                {
                    "voxel_offset": [0, 0, 0],
                    "key": "0_0_0",
                    "size": data_shape,
                    "resolution": [727.8, 727.8, 727.8],     # Resolution in nm
                    "chunk_sizes": [args.phase1chunks[::-1]],
                    "encoding": "raw",
                }
            ]
        }

    out_vol = CloudVolume(args.output_name, volume_info=volume_info, bounded=False, progress=True, non_aligned_writes=False, parallel=1)
    out_vol.provenance.description = f"Marker prediction generated by Pi2/Monai UNet model"
    out_vol.provenance.owners = ["jonas.hemesath@bi.mpg.de"]
    out_vol.commit_info()
    out_vol.commit_provenance()



x_chunks = math.ceil(data_shape[0]/stride[0])
y_chunks = math.ceil(data_shape[1]/stride[1])
z_chunks = math.ceil(data_shape[2]/stride[2])

if args.testing_x_loc is None:
    test_x = None
else:
    test_x = math.ceil(args.testing_x_loc/stride[0])
print(test_x)

total_jobs = x_chunks * y_chunks * z_chunks
print(f"Launching {total_jobs} jobs ({x_chunks}x{y_chunks}x{z_chunks})")

processes = []
process_id = 0

for x_i in [idx for idx in range(args.step)]:
    for y_i in [idx for idx in range(args.step)]:
        for z_i in [idx for idx in range(args.step)]:
            processes = []
            for x in range(x_i, x_chunks, args.step):
                if test_x is not None and x != test_x:
                    print('Skipping x =', x)
                    continue
                x_org = x * stride[0]
                block_x = min(args.block_shape[0], args.dataset_shape[0]-x_org)
                for y in range(y_i, y_chunks, args.step):
                    y_org = y * stride[1]
                    block_y = min(args.block_shape[1], args.dataset_shape[1]-y_org)
                    for z in range(z_i, z_chunks, args.step):
                        z_org = z * stride[2]
                        block_z = min(args.block_shape[2], args.dataset_shape[2]-z_org)
                        if args.export_debug:
                            processes.append(subprocess.Popen(['srun', '--time=7-0', '--gres=gpu:a40:1', '--mem=400000', '--tasks', '1', '--cpus-per-task', '32', 'python', '/cajal/nvmescratch/users/johem/esrf_data_conversion/segmentation/infer_block_semantic_multires.py',
                                                            '--predict_image', args.data_path,
                                                            '--output_dir', args.output_name,
                                                            '--attention', str(args.attention),
                                                            '--mode', args.mode,
                                                            '--block_origin', str(x_org), str(y_org), str(z_org),
                                                            '--block_shape', str(block_x), str(block_y), str(block_z),
                                                            '--model_path', args.model_path,
                                                            '--process_id', str(process_id),
                                                            '--debug_path', debug_path,
                                                            '--ds_levels'] + [str(ds) for ds in args.ds_levels],
                                                            stdout=subprocess.PIPE, stderr=subprocess.PIPE))
                        else:
                            processes.append(subprocess.Popen(['srun', '--time=7-0', '--gres=gpu:a40:1', '--mem=400000', '--tasks', '1', '--cpus-per-task', '32', 'python', '/cajal/nvmescratch/users/johem/esrf_data_conversion/segmentation/infer_block_semantic_multires.py',
                                                            '--predict_image', args.data_path,
                                                            '--output_dir', args.output_name,
                                                            '--attention', str(args.attention),
                                                            '--mode', args.mode,
                                                            '--block_origin', str(x_org), str(y_org), str(z_org),
                                                            '--block_shape', str(block_x), str(block_y), str(block_z),
                                                            '--model_path', args.model_path,
                                                            '--process_id', str(process_id),
                                                            '--ds_levels'] + [str(ds) for ds in args.ds_levels],
                                                            stdout=subprocess.PIPE, stderr=subprocess.PIPE))
                        process_id += 1
            

            for i, process in enumerate(processes):
                outs, errs = process.communicate()
                if errs:
                    print(f"Process {i+1} errors:")
                    print(errs.decode('utf-8') if isinstance(errs, bytes) else errs)
                if outs:
                    print(f"Process {i+1} output:")
                    print(outs.decode('utf-8') if isinstance(outs, bytes) else outs)
                print('Process', i+1, 'of', len(processes), 'done')



print('All done')
print('Took', round(time.time()-t0), 's')

